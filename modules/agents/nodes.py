"""
LangGraph èŠ‚ç‚¹æ¨¡å—
"""

import streamlit as st
import base64
import time
import difflib
import re
from typing import Any, Dict, List
from datetime import datetime
from langchain_core.messages import HumanMessage, SystemMessage

from modules.utils.state import HTPState, add_chat_message, add_conversation_entry
from modules.prompts.manager import get_prompt_manager
from modules.surveys.dass21 import calculate_dass21_scores, format_dass21_results


def _get_qa_config(state) -> Dict[str, Any]:
    """å®‰å…¨è¯»å– QA é…ç½®ï¼Œå¸¦é»˜è®¤å›é€€ã€‚"""
    default = {
        "max_questions_per_category": 2,
        "max_total_questions": 8,
        "enable_smart_coverage": True,
        "coverage_threshold": 0.7,
        "dedupe_similarity_threshold": 0.88,
    }
    cfg = state.get("qa_config") or {}
    return {**default, **cfg}


CATEGORY_TOPICS: Dict[str, List[Dict[str, Any]]] = {
    "person": [
        {"name": "èº«ä»½/äººç‰©æ˜¯è°", "keywords": ["æ˜¯è°", "ä½ å—", "åˆ«äºº", "ä»–", "å¥¹"]},
        {"name": "è¡Œä¸º/æ­£åœ¨åšä»€ä¹ˆ", "keywords": ["åšä»€ä¹ˆ", "åœ¨åš", "æ­£åœ¨", "åŠ¨ä½œ"]},
        {
            "name": "æƒ…ç»ª/æ„Ÿè§‰",
            "keywords": ["å¼€å¿ƒ", "å®³æ€•", "ç´§å¼ ", "æƒ…ç»ª", "æ„Ÿè§‰", "æ„Ÿå—"],
        },
        {"name": "é™ªä¼´/ç¤¾äº¤", "keywords": ["æœ‰äºº", "é™ª", "æœ‹å‹", "å­¤ç‹¬", "é™ªä¼´"]},
        {"name": "ç©¿ç€/ä¸ªæ€§", "keywords": ["è¡£æœ", "ç©¿", "ä¸ªæ€§", "é£æ ¼"]},
        {"name": "å†…å¿ƒç‹¬ç™½", "keywords": ["è¯´ä»€ä¹ˆ", "ä¼šè¯´", "ä¸€å¥è¯"]},
    ],
    "house": [
        {"name": "ä½ç½®/ç¯å¢ƒ", "keywords": ["åŸé‡Œ", "éƒŠå¤–", "ä½ç½®", "ç¯å¢ƒ"]},
        {"name": "å½’å±/æ‰€æœ‰æƒ", "keywords": ["å±äº", "ä½ çš„", "è°çš„", "è®¤è¯†çš„äººçš„"]},
        {"name": "å†…éƒ¨æ˜¯å¦æœ‰äºº/å¸Œæœ›", "keywords": ["é‡Œé¢", "æœ‰äºº", "è°ä½", "å¸Œæœ›è°"]},
        {"name": "é—¨å¼€åˆ/æ¥çº³", "keywords": ["é—¨", "å¼€", "å…³", "èµ°è¿›æ¥"]},
        {"name": "å¤§å°/é€‚é…", "keywords": ["å¤§", "å°", "åˆšåˆšå¥½", "å¤ªå¤§", "å¤ªå°"]},
        {"name": "åŠŸèƒ½/ä¿æŠ¤ä¸å±•ç¤º", "keywords": ["ä¿æŠ¤", "å±•ç¤º"]},
        {"name": "æ‹Ÿäººè¡¨è¾¾", "keywords": ["è¯´è¯", "ä¼šè¯´ä»€ä¹ˆ"]},
    ],
    "tree": [
        {"name": "æ¥æº/è®°å¿†", "keywords": ["è§è¿‡", "æ¥è‡ªå“ªé‡Œ", "å“ªé‡Œ", "ä»å“ª"]},
        {"name": "å¥åº·/ç»å†", "keywords": ["å¥åº·", "ç»å†", "ç»å†è¿‡", "æˆé•¿"]},
        {"name": "ç‰¹å¾/å¶æœ", "keywords": ["å¶å­", "æœå®", "ç‰¹åˆ«", "æ"]},
        {"name": "æ ¹/ç¨³å®š", "keywords": ["æ ¹", "æ‰", "ç¨³", "ç¨³å®š"]},
        {"name": "è±¡å¾/è‡ªæˆ‘æŠ•å°„", "keywords": ["è±¡å¾", "å“ªéƒ¨åˆ†", "ä½ çš„ä¸€éƒ¨åˆ†"]},
        {"name": "å­¤ç‹¬/åŒä¼´", "keywords": ["å­¤ç‹¬", "æœ‹å‹", "åˆ«çš„æ ‘"]},
    ],
    "overall": [
        {"name": "åˆ›ä½œæ—¶æƒ³æ³•", "keywords": ["æƒ³ä»€ä¹ˆ", "åœ¨æƒ³", "åˆ›ä½œ"]},
        {"name": "ä»£è¡¨è¿‘æœŸçŠ¶æ€", "keywords": ["ä»£è¡¨", "æœ€è¿‘", "çŠ¶æ€"]},
        {"name": "æ°›å›´/æƒ…ç»ª", "keywords": ["æ°›å›´", "å¹³é™", "è‡ªç”±", "ç´§å¼ ", "æ„Ÿè§‰"]},
        {"name": "ç©ºç™½/æ„ä¹‰", "keywords": ["ç©ºç™½", "æ„ä¹‰", "ç‰¹åˆ«"]},
        {"name": "è‰²å½©/æƒ…ç»ª", "keywords": ["è‰²å½©", "é¢œè‰²", "æƒ…ç»ª"]},
        {"name": "é‡ç‚¹éƒ¨ä½/åŸå› ", "keywords": ["ç‰¹åˆ«æƒ³", "éƒ¨åˆ†", "ä¸ºä»€ä¹ˆ"]},
    ],
}


def _normalize(text: str) -> str:
    text = text or ""
    text = text.lower()
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def _similar(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, _normalize(a), _normalize(b)).ratio()


def _collect_category_qa(state, category: str) -> List[Dict[str, str]]:
    return [
        e
        for e in state.get("conversation_history", [])
        if e.get("category") == category
    ]


def _build_context_for_prompt(state, category: str) -> Dict[str, str]:
    qa_entries = _collect_category_qa(state, category)
    asked_questions = [e["question"] for e in qa_entries if e.get("question")]
    qa_pairs_lines = []
    for e in qa_entries:
        q = e.get("question", "").strip()
        r = e.get("response", "").strip()
        if q or r:
            qa_pairs_lines.append(f"Q: {q}\nA: {r}")

    asked_questions_str = "\n".join(asked_questions[-6:])  # ä»…ä¿ç•™è¿‘6æ¡
    qa_pairs_str = "\n\n".join(qa_pairs_lines[-6:])

    # è®¡ç®—æœªè¦†ç›–çš„ä¸»é¢˜ï¼ˆåŸºäºå¯å‘å¼å…³é”®è¯ï¼‰
    combined_text = (
        "\n".join(asked_questions)
        + "\n"
        + "\n".join([e.get("response", "") for e in qa_entries])
    ).strip()
    topics = CATEGORY_TOPICS.get(category, [])
    covered = set()
    for t in topics:
        if any(k.lower() in combined_text.lower() for k in t["keywords"]):
            covered.add(t["name"])
    missing = [t["name"] for t in topics if t["name"] not in covered]
    missing_aspects_str = (
        "ã€".join(missing) if missing else "ï¼ˆå·²è¦†ç›–ä¸»è¦æ–¹å‘ï¼Œå¯æ”¶æŸï¼‰"
    )

    return {
        "asked_questions": asked_questions_str,
        "qa_pairs": qa_pairs_str,
        "missing_aspects": missing_aspects_str,
    }


def _should_mark_category_complete(state, category: str) -> bool:
    cfg = _get_qa_config(state)
    max_per_cat = int(cfg.get("max_questions_per_category", 2))
    entries = _collect_category_qa(state, category)
    if len(entries) >= max_per_cat:
        return True

    if not cfg.get("enable_smart_coverage", True):
        return False

    # ç®€å•å¯å‘å¼ï¼šå¦‚æœæœ€è¿‘ä¸€æ¬¡å›ç­”ä¸å†å²å›ç­”ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œæˆ–ä¿¡æ¯å¢é‡å¾ˆä½ï¼Œåˆ™è®¤ä¸ºå·²åŸºæœ¬è¦†ç›–
    responses = [e.get("response", "") for e in entries]
    if len(responses) < 1:
        return False
    combined_prev = "\n".join(responses[:-1]) if len(responses) > 1 else ""
    last = responses[-1]
    if not combined_prev:
        return False
    sim = _similar(last, combined_prev)
    # ç›¸ä¼¼åº¦é«˜è§†ä¸ºæ”¶æ•›
    if sim >= max(0.6, cfg.get("coverage_threshold", 0.7) - 0.1):
        return True
    return False


def analyze_image_node(state: HTPState, llm: Any) -> HTPState:
    """
    å›¾åƒåˆ†æèŠ‚ç‚¹

    Args:
        state: HTP çŠ¶æ€
        llm: LLM æ¨¡å‹

    Returns:
        HTPState: æ›´æ–°åçš„çŠ¶æ€
    """
    if state["image_analysis"]:
        return state

    prompt_manager = get_prompt_manager()
    image_prompt = prompt_manager.load_prompt_from_yaml("01_image_analysis.yaml")

    if not image_prompt:
        st.error("æ— æ³•åŠ è½½å›¾åƒåˆ†æ prompt")
        return state

    if not state["uploaded_image"]:
        st.error("æœªæ‰¾åˆ°ä¸Šä¼ çš„å›¾åƒ")
        return state

    try:
        with st.spinner("ğŸ” æ­£åœ¨åˆ†æå›¾åƒå†…å®¹..."):
            messages = [
                SystemMessage(content=image_prompt.format()),
                HumanMessage(
                    content=[
                        {"type": "text", "text": "è¯·åˆ†æè¿™å¼ HTPç»˜ç”»å›¾åƒï¼š"},
                        {
                            "type": "image",
                            "image": {
                                "format": "png",
                                "source": {
                                    "bytes": base64.b64decode(state["uploaded_image"])
                                },
                            },
                        },
                    ]
                ),
            ]

            response = llm.invoke(messages)
            analysis = response.content

            state["image_analysis"] = analysis
            state["current_stage"] = "qa_loop"
            state["stage_progress"]["image_analysis"] = True

            # æ·»åŠ åˆ°èŠå¤©è®°å½•
            state = add_chat_message(
                state, "assistant", f"âœ… å›¾åƒåˆ†æå®Œæˆï¼\n\n**åˆ†æç»“æœï¼š**\n{analysis}"
            )

            st.success("âœ… å›¾åƒåˆ†æå®Œæˆ")

    except Exception as e:
        st.error(f"å›¾åƒåˆ†æå¤±è´¥: {str(e)}")

    return state


def generate_question_node(state: HTPState, llm: Any) -> HTPState:
    """
    ç”Ÿæˆé—®é¢˜èŠ‚ç‚¹

    Args:
        state: HTP çŠ¶æ€
        llm: LLM æ¨¡å‹

    Returns:
        HTPState: æ›´æ–°åçš„çŠ¶æ€
    """
    category = state["current_category"]
    if not category:
        return state

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°è¯¥ç±»åˆ«æˆ–å…¨å±€çš„æœ€å¤§é—®é¢˜æ•°
    cfg = _get_qa_config(state)
    max_questions = int(cfg.get("max_questions_per_category", 2))
    max_total = int(cfg.get("max_total_questions", 8))
    current_responses = len(state["collected_info"].get(category, []))

    # å…¨å±€ä¸Šé™ä¿æŠ¤
    if state.get("total_questions_asked", 0) >= max_total:
        state["categories_covered"].append(category)
        state["current_category"] = None
        return state

    if current_responses >= max_questions:
        state["categories_covered"].append(category)
        state["current_category"] = None
        return state

    # ç”Ÿæˆé—®é¢˜
    try:
        prompt_manager = get_prompt_manager()
        prompt_files = prompt_manager.get_question_prompt_files()

        question_prompt = prompt_manager.load_prompt_from_yaml(prompt_files[category])
        if not question_prompt:
            st.error(f"æ— æ³•åŠ è½½ {category} é—®é¢˜ prompt")
            return state

        # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆåŒ…å«å†å²é—®ç­”ã€å·²é—®é—®é¢˜ã€ç¼ºå¤±æ–¹é¢ï¼‰
        rich_ctx = _build_context_for_prompt(state, category)
        context_parts = []
        if rich_ctx.get("qa_pairs"):
            context_parts.append("ä¹‹å‰çš„é—®ç­”ï¼š\n" + rich_ctx["qa_pairs"])
        if rich_ctx.get("asked_questions"):
            context_parts.append(
                "ä¹‹å‰é—®è¿‡çš„é—®é¢˜ï¼ˆé¿å…é‡å¤ï¼‰ï¼š\n" + rich_ctx["asked_questions"]
            )
        if rich_ctx.get("missing_aspects"):
            context_parts.append(
                "å°šæœªå……åˆ†æ¢ç´¢çš„æ–¹å‘ï¼ˆå¯ä¼˜å…ˆè€ƒè™‘ï¼‰ï¼š\n" + rich_ctx["missing_aspects"]
            )
        context = "\n\n".join(context_parts).strip()

        with st.spinner(f"ğŸ¤” æ­£åœ¨ç”Ÿæˆ{category}ç›¸å…³é—®é¢˜..."):
            formatted_prompt = question_prompt.format(
                image_analysis=state["image_analysis"], context=context
            )

            def generate_once(extra_instruction: str = "") -> str:
                messages = [
                    SystemMessage(content=formatted_prompt),
                    HumanMessage(
                        content=(
                            "è¯·ç”Ÿæˆä¸€ä¸ªåˆé€‚çš„é—®é¢˜ï¼Œèšç„¦æœªè¦†ç›–çš„æ–¹å‘ï¼Œ"
                            "å¹¶ä¸”é¿å…ä¸å…ˆå‰é—®ç­”é‡å¤æˆ–ç±»ä¼¼çš„é—®é¢˜ã€‚"
                            + (f" {extra_instruction}" if extra_instruction else "")
                        )
                    ),
                ]
                resp = llm.invoke(messages)
                return (resp.content or "").strip()

            # é¦–æ¬¡ç”Ÿæˆ
            new_question = generate_once()

            # å»é‡ï¼šä¸å†å²å·²é—®é—®é¢˜æ¯”è¾ƒç›¸ä¼¼åº¦
            qa_entries = _collect_category_qa(state, category)
            asked_questions = [
                e.get("question", "") for e in qa_entries if e.get("question")
            ]
            threshold = float(cfg.get("dedupe_similarity_threshold", 0.88))
            retry = 0
            max_retries = 2
            while retry < max_retries and any(
                _similar(new_question, q) >= threshold for q in asked_questions
            ):
                retry += 1
                new_question = generate_once(
                    "ä¸Šä¸ªææ¡ˆä¸æ—¢æœ‰é—®é¢˜è¿‡äºç›¸ä¼¼ï¼Œè¯·æ¢ä¸€ä¸ªè§’åº¦æˆ–ä¸»é¢˜ã€‚é¿å…ä¸ä»»ä½•å†å²é—®é¢˜é‡å¤ã€‚"
                )

            # ä»ç„¶è¿‡äºç›¸ä¼¼ï¼Œåˆ™åŸºäºç¼ºå¤±ä¸»é¢˜ç›´æ¥æ„å»ºä¸€ä¸ª fallback é—®å¥
            if any(_similar(new_question, q) >= threshold for q in asked_questions):
                missing = (
                    rich_ctx.get("missing_aspects", "")
                    .replace("ï¼ˆå·²è¦†ç›–ä¸»è¦æ–¹å‘ï¼Œå¯æ”¶æŸï¼‰", "")
                    .strip()
                )
                if missing:
                    new_question = f"å…³äºä½ ç”»ä½œä¸­å°šæœªå……åˆ†è°ˆåˆ°çš„â€˜{missing.split('ã€')[0]}â€™è¿™ä¸€é¢ï¼Œä½ æ„¿æ„å¤šåˆ†äº«ä¸€ç‚¹å—ï¼Ÿ"

            state["current_question"] = new_question
            state["total_questions_asked"] += 1
            state["waiting_for_user_input"] = True

            # æ·»åŠ åˆ°èŠå¤©è®°å½•
            category_names = {
                "person": "ğŸ‘¤ äººç‰©",
                "house": "ğŸ  æˆ¿å­",
                "tree": "ğŸŒ³ æ ‘æœ¨",
                "overall": "ğŸ¨ æ•´ä½“æ„Ÿå—",
            }

            state = add_chat_message(
                state,
                "assistant",
                f"**{category_names[category]} ç›¸å…³é—®é¢˜ï¼š**\n\n{new_question}",
            )

    except Exception as e:
        st.error(f"é—®é¢˜ç”Ÿæˆå¤±è´¥: {str(e)}")

    return state


def decide_next_category_node(state: HTPState) -> str:
    """
    å†³å®šä¸‹ä¸€ä¸ªé—®ç­”ç±»åˆ«

    Args:
        state: HTP çŠ¶æ€

    Returns:
        str: ä¸‹ä¸€æ­¥è¡ŒåŠ¨
    """
    cfg = _get_qa_config(state)
    categories = ["person", "house", "tree", "overall"]

    # å¦‚æœå…¨å±€é—®é¢˜æ•°å·²è¾¾ä¸Šé™ï¼Œåˆ™ç›´æ¥è¿›å…¥åˆ†æé˜¶æ®µ
    if state.get("total_questions_asked", 0) >= int(cfg.get("max_total_questions", 8)):
        state["is_qa_complete"] = True
        state["current_stage"] = "category_analysis"
        return "category_analysis"

    for category in categories:
        if category in state["categories_covered"]:
            continue
        # è‹¥å·²æœ‰å¯¹è¯ä¸”è¦†ç›–åº¦è¾¾åˆ°é˜ˆå€¼ï¼Œåˆ™æ ‡è®°å®Œæˆå¹¶è·³è¿‡
        if _should_mark_category_complete(state, category):
            if category not in state["categories_covered"]:
                state["categories_covered"].append(category)
            continue
        state["current_category"] = category
        return "generate_question"

    # æ‰€æœ‰ç±»åˆ«éƒ½å®Œæˆäº†
    state["is_qa_complete"] = True
    state["current_stage"] = "category_analysis"
    return "category_analysis"


def process_user_response_node(state: HTPState, user_response: str) -> HTPState:
    """
    å¤„ç†ç”¨æˆ·å›ç­”èŠ‚ç‚¹

    Args:
        state: HTP çŠ¶æ€
        user_response: ç”¨æˆ·å›ç­”

    Returns:
        HTPState: æ›´æ–°åçš„çŠ¶æ€
    """
    if not state["current_category"] or not user_response.strip():
        return state

    category = state["current_category"]

    # åˆå§‹åŒ–ç±»åˆ«ä¿¡æ¯æ”¶é›†
    if category not in state["collected_info"]:
        state["collected_info"][category] = []

    # æ·»åŠ åˆ°å¯¹è¯è®°å½•å’Œæ”¶é›†ä¿¡æ¯
    state = add_conversation_entry(
        state, category, state["current_question"], user_response
    )

    # æ·»åŠ åˆ°èŠå¤©è®°å½•
    state = add_chat_message(state, "user", user_response)

    # æ¸…é™¤å½“å‰é—®é¢˜
    state["current_question"] = None
    state["waiting_for_user_input"] = False

    return state


def category_analysis_node(state: HTPState, llm: Any) -> HTPState:
    """
    å„ç±»åˆ«åˆ†æèŠ‚ç‚¹

    Args:
        state: HTP çŠ¶æ€
        llm: LLM æ¨¡å‹

    Returns:
        HTPState: æ›´æ–°åçš„çŠ¶æ€
    """
    if not state["stage_progress"].get("category_analysis", False):

        with st.spinner("ğŸ“ æ­£åœ¨è¿›è¡Œå„ç±»åˆ«è¯¦ç»†åˆ†æ..."):

            prompt_manager = get_prompt_manager()
            prompt_files = prompt_manager.get_analysis_prompt_files()

            for category in ["person", "house", "tree", "overall"]:
                if (
                    category in state["collected_info"]
                    and state["collected_info"][category]
                ):
                    try:
                        # è·å–è¯¥ç±»åˆ«çš„åˆ†æprompt
                        analysis_prompt = prompt_manager.load_prompt_from_yaml(
                            prompt_files[category]
                        )
                        if not analysis_prompt:
                            continue

                        # å‡†å¤‡æ•°æ®
                        responses = "\n".join(state["collected_info"][category])

                        formatted_prompt = analysis_prompt.format(
                            image_analysis=state["image_analysis"],
                            **{f"{category}_responses": responses},
                        )

                        messages = [
                            SystemMessage(content=formatted_prompt),
                            HumanMessage(content="è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯è¿›è¡Œåˆ†æã€‚"),
                        ]

                        response = llm.invoke(messages)
                        state["category_analyses"][category] = response.content

                        time.sleep(0.5)  # é¿å…è¿‡å¿«è°ƒç”¨API

                    except Exception as e:
                        st.error(f"{category}ç±»åˆ«åˆ†æå¤±è´¥: {str(e)}")
                        state["category_analyses"][category] = f"åˆ†æå¤±è´¥: {str(e)}"

        state["stage_progress"]["category_analysis"] = True
        state["current_stage"] = "comprehensive"

        # æ·»åŠ åˆ°èŠå¤©è®°å½•
        state = add_chat_message(
            state, "assistant", "âœ… å„ç±»åˆ«åˆ†æå®Œæˆï¼Œæ­£åœ¨è¿›è¡Œç»¼åˆåˆ†æ..."
        )

        st.success("âœ… å„ç±»åˆ«åˆ†æå®Œæˆ")

    return state


def comprehensive_analysis_node(state: HTPState, llm: Any) -> HTPState:
    """
    ç»¼åˆåˆ†æèŠ‚ç‚¹

    Args:
        state: HTP çŠ¶æ€
        llm: LLM æ¨¡å‹

    Returns:
        HTPState: æ›´æ–°åçš„çŠ¶æ€
    """
    if not state["stage_progress"].get("comprehensive", False):

        try:
            with st.spinner("ğŸ§  æ­£åœ¨è¿›è¡Œç»¼åˆæ•´åˆåˆ†æ..."):
                # åŠ è½½ç»¼åˆåˆ†æprompt
                prompt_manager = get_prompt_manager()
                comprehensive_prompt = prompt_manager.load_prompt_from_yaml(
                    "10_comprehensive_analysis.yaml"
                )
                if not comprehensive_prompt:
                    st.error("æ— æ³•åŠ è½½ç»¼åˆåˆ†æ prompt")
                    return state

                # å‡†å¤‡æ‰€æœ‰æ•°æ®
                prompt_data = {
                    "image_analysis": state["image_analysis"],
                    "person_responses": "\n".join(
                        state["collected_info"].get("person", [])
                    ),
                    "house_responses": "\n".join(
                        state["collected_info"].get("house", [])
                    ),
                    "tree_responses": "\n".join(
                        state["collected_info"].get("tree", [])
                    ),
                    "overall_responses": "\n".join(
                        state["collected_info"].get("overall", [])
                    ),
                    "person_analysis": state["category_analyses"].get("person", ""),
                    "house_analysis": state["category_analyses"].get("house", ""),
                    "tree_analysis": state["category_analyses"].get("tree", ""),
                    "overall_analysis": state["category_analyses"].get("overall", ""),
                }

                formatted_prompt = comprehensive_prompt.format(**prompt_data)

                messages = [
                    SystemMessage(content=formatted_prompt),
                    HumanMessage(content="è¯·è¿›è¡Œç»¼åˆæ•´åˆåˆ†æã€‚"),
                ]

                response = llm.invoke(messages)
                state["comprehensive_analysis"] = response.content

                state["stage_progress"]["comprehensive"] = True
                state["current_stage"] = "dass21"

                # æ·»åŠ åˆ°èŠå¤©è®°å½•
                state = add_chat_message(
                    state, "assistant", "âœ… ç»¼åˆåˆ†æå®Œæˆï¼è¯·ç»§ç»­å®Œæˆ DASS-21 é‡è¡¨è¯„ä¼°ã€‚"
                )

                st.success("âœ… ç»¼åˆåˆ†æå®Œæˆï¼Œå‡†å¤‡è¿›å…¥é‡è¡¨è¯„ä¼°")

        except Exception as e:
            st.error(f"ç»¼åˆåˆ†æå¤±è´¥: {str(e)}")

    return state


def process_dass21_node(state: HTPState, responses: dict) -> HTPState:
    """
    å¤„ç†DASS-21é—®å·ç»“æœèŠ‚ç‚¹

    Args:
        state: HTP çŠ¶æ€
        responses: DASS-21 å›ç­”

    Returns:
        HTPState: æ›´æ–°åçš„çŠ¶æ€
    """
    if responses and len(responses) == 21:
        try:
            # è®¡ç®—å¾—åˆ†å’Œç­‰çº§
            scores, levels = calculate_dass21_scores(responses)

            # æ›´æ–°çŠ¶æ€
            state["dass21_responses"] = responses
            state["dass21_scores"] = scores
            state["dass21_levels"] = levels
            state["is_dass21_complete"] = True
            state["stage_progress"]["dass21"] = True
            state["current_stage"] = "final_report"

            # æ·»åŠ åˆ°èŠå¤©è®°å½•
            state = add_chat_message(
                state,
                "assistant",
                f"âœ… DASS-21 é‡è¡¨è¯„ä¼°å®Œæˆï¼\n\n{format_dass21_results(scores, levels)}",
            )

            st.success("âœ… DASS-21 é‡è¡¨è¯„ä¼°å®Œæˆ")

        except Exception as e:
            st.error(f"DASS-21 è¯„ä¼°å¤„ç†å¤±è´¥: {str(e)}")

    return state


def final_report_node(state: HTPState, llm: Any) -> HTPState:
    """
    æœ€ç»ˆæŠ¥å‘Šç”ŸæˆèŠ‚ç‚¹

    Args:
        state: HTP çŠ¶æ€
        llm: LLM æ¨¡å‹

    Returns:
        HTPState: æ›´æ–°åçš„çŠ¶æ€
    """
    if not state["stage_progress"].get("final_report", False):

        try:
            with st.spinner("ğŸ“Š æ­£åœ¨ç”Ÿæˆæœ€ç»ˆå¿ƒç†è¯„ä¼°æŠ¥å‘Š..."):
                # åŠ è½½æœ€ç»ˆè¯„ä¼°prompt
                prompt_manager = get_prompt_manager()
                final_prompt = prompt_manager.load_prompt_from_yaml(
                    "11_final_evaluation.yaml"
                )
                if not final_prompt:
                    st.error("æ— æ³•åŠ è½½æœ€ç»ˆè¯„ä¼° prompt")
                    return state

                # å‡†å¤‡å®Œæ•´æ•°æ®
                conversation_summary = ""
                for entry in state["conversation_history"]:
                    conversation_summary += f"[{entry['category']}] Q: {entry['question']}\nA: {entry['response']}\n\n"

                # æ ¼å¼åŒ–DASS-21ç»“æœ
                dass21_summary = ""
                if state.get("dass21_scores") and state.get("dass21_levels"):
                    dass21_summary = format_dass21_results(
                        state["dass21_scores"], state["dass21_levels"]
                    )

                prompt_data = {
                    "image_analysis": state["image_analysis"],
                    "comprehensive_analysis": state["comprehensive_analysis"],
                    "person_analysis": state["category_analyses"].get("person", ""),
                    "house_analysis": state["category_analyses"].get("house", ""),
                    "tree_analysis": state["category_analyses"].get("tree", ""),
                    "overall_analysis": state["category_analyses"].get("overall", ""),
                    "conversation_history": conversation_summary,
                    "dass21_results": dass21_summary,
                }

                formatted_prompt = final_prompt.format(**prompt_data)

                messages = [
                    SystemMessage(content=formatted_prompt),
                    HumanMessage(content="è¯·ç”Ÿæˆå®Œæ•´çš„å¿ƒç†çŠ¶æ€å¤šç»´åº¦è¯„ä¼°æŠ¥å‘Šã€‚"),
                ]

                response = llm.invoke(messages)
                state["final_report"] = response.content

                state["stage_progress"]["final_report"] = True
                state["analysis_timestamp"] = datetime.now().isoformat()

                # æ·»åŠ åˆ°èŠå¤©è®°å½•
                state = add_chat_message(
                    state,
                    "assistant",
                    "ğŸ‰ å®Œæ•´çš„å¿ƒç†è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆï¼è¯·æŸ¥çœ‹æŠ¥å‘Šæ ‡ç­¾é¡µã€‚",
                )

                st.success("âœ… å®Œæ•´æŠ¥å‘Šç”Ÿæˆå®Œæˆ")

        except Exception as e:
            st.error(f"æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")

    return state
